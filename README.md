# BreastCancerDiagnosis
### 介紹
此專案中我們針對細針抽吸方式 FNA（fine needle aspirate)獲得的乳腺癌檢測的患者數據集，建立一預測模型。資料集的來源是開源資料集-[乳腺癌數據集](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))，每個患者的FNA數據是由多個細胞樣本中產生 10 個不同特徵的統計數據以及診斷結果（惡性或良性）表示。基於多篇論文、醫學專家下，在[Eickhoff (2016)](https://dl.acm.org/doi/pdf/10.1145/2594776.2594788?casa_token=GMtjoBep2nkAAAAA:7n4D47l-D5yDvNTHgw8KBqQwQd03KuJnYy3hXhBTKqv940MklIJFSsM0wuF4JA1wnL0qv3K3YDp_7g)論文中整理了5個判斷為惡性腫瘤的要點，我們根據這5要點作為模型設計、訓練的基準。分別在「Rule-Based」、「隨機森林分類器」、「邏輯回歸分類器」下執行多項實驗。經過優化最後「隨機森林分類器」、「邏輯回歸分類器」獲得了相似好的表現，<b> Avg precision: 97%，Avg recall: 96，Avg f1-score: 97</b>。


### 介紹
[Eickhoff (2016)](https://dl.acm.org/doi/pdf/10.1145/2594776.2594788?casa_token=GMtjoBep2nkAAAAA:7n4D47l-D5yDvNTHgw8KBqQwQd03KuJnYy3hXhBTKqv940MklIJFSsM0wuF4JA1wnL0qv3K3YDp_7g)論文中基於「考慮到特定類型的組織，細胞的大小往往是均勻的」所設計的五要點為：
1) 存在明顯較大的細胞是不受控制的生長的證據，這表明惡性腫瘤。
2) 良性細胞的形狀通常僅顯示有限的差異，而惡性細胞可以發展出與其周圍環境的一般模式不相符的任意結構。
3) 相同類型的常規細胞的細胞核顏色應相同。癌細胞通常具有明顯更大且較暗的細胞核，DNA 更密集。
4) 規則細胞顯示出相似的紋理。另一方面，惡性腫瘤可以範圍從光滑的表面到相鄰細胞的參差不齊或塊狀紋理。
5) 對於健康組織，細胞排列趨於有序，細胞之間的距離規則。癌細胞幾乎可以任意散開或雜亂無章。

### 實驗
#### （ㄧ）Rule-Based
在分配診斷惡性或良性的任務中實施和評估以下分類器：
遵循以下形式的基於規則的分類器：
  如果
  1. 【單元格大小異常】：
  2. 或【細胞形狀異常】
  3. 或【細胞紋理異常】
  4. 或【細胞相似性/同質性異常】
  - 那麼：診斷為<b>惡性</b>， 
  - 否則：診斷為<b>良性</b>。  
  

基於Rule-Based的第一個分類器，我們選擇<b>度量區域面積、周長和凹度</b>作為基線來定義患者是否為良性。 最初，選擇的度量標準是平滑度和紋理，這並沒有產生差異化的值範圍。 惡性和良性患者的這些指標的值往往類似。
因此，我們選擇了其他指標來確定模型分類器的性能。這些值是在提供的數據集上計算的。
我們計算這些字段的<b>平均值、最小值和最大值</b>，並根據計算值確定患者如何分類為良性和惡性類別。 根據平均值和最差面積/凹度進行計算。 我們為我們得到的計算提供了一個片段.   
得到下表數據  
|  惡性   | Area  | Perimeter  |Concavity |
|  ----  | ----  |----  |---- |
|  Mean  | 978.38 | 115.37    |0.16  |
|  Min  | 361.6   | 71.9      |0.02  |
|  Max  | 2501    | 188.5     |0.42  |  

| 良性    |  Area | Perimeter  |Concavity |
|  ----  | ----  |----  |---- |
|  Mean  | 462.79 | 78.07    |0.04  |
|  Min   | 143.5   | 43.79   |0.0  |
|  Max   | 992    | 114.6    |0.41  |

在Rule-Based分類器中，模型由給定的規則和算法構建。 我們計算最小值、平均值、最大值來判斷數據是否異常，就像一個專家系統。根據我們的實驗結果它的性能遠不如其他 ML 模型，儘管它的條件值是由統計值給出的。 因為當我們計算數據集中某些列的平均值時，我們發現它們的值非常相似，因此我們不使用該列作為我們算法的條件。 事實上，雖然它們的價值觀非常相似，但它們仍然有一些我們不知道的不同之處。 如果我們考慮更多的條件，它會更精確。 <b>基於規則的模型的優點是它有清晰的算法讓我們知道模型是如何做的，但它的缺點是它沒有考慮所有的情況</b>。

#### （二）隨機森林分類器
sklearn 框架的隨機森林分類器應用於提供的數據集中給定的特徵。  

在 RF 分類器中，由給定​​數據集構建的模型找到了一個很好的解決方案來分類數據是否是惡性的。這是一個常見的白盒解決方案，在 RF 中有很多決策樹。每棵樹都有一個規則，可以區分數據並投票給最終的標籤。在我們的實驗中，RF 獲得了很好的Accuracy，0.96。 RF的良性召Recall為0.98，惡性Recall為0.94。這意味著模型錯誤發現良性的能力優於惡性。
RF 是基於決策樹的隨機森林分類器。這個想法是通過平均它們的結果來減少幾個嘈雜決策樹的預測方差。在可解釋性方面，介於傳統機器學習模型和深度學習之間。由於有很多樹，數量使它更像黑匣子。在 Ahsan Saeed 實驗中，他們發現在低深度時，隨機森林中的樹木往往相似，因此我們看到了高度的正相關。相關性隨著深度的增加而降低，因為樹開始在不同的特徵上分裂。重要的是，我們看到的模式
    
指示在每個深度級別的集群的潛在形成。此外，他通過使用 t-SNE 證明了不同深度級別的集群形成。因此，當我們使用 RF 模型時，它將數據集劃分為樹的數量，在深度增長後，樹將傾向於彼此聚集。關鍵思想是隨機森林中的底層決策樹可以展示集群。一旦我們對集群有信心，最後一步就是從每個集群中挑選一個具有代表性的未修剪樹並將其呈現給客戶端。  


| Test Data Report   | Precision | Recall | F1-Score |
|  ----  | ----      | ----   |----       |
|  良性               | 0.96      | 0.98   |0.97     |
|  惡性               | 0. 97     | 0.94   |0.95     |
|  accuracy          |           |        |0.96     |

#### （三）邏輯回歸分類器 
邏輯回歸使用所有輸入的變量，並在使用Logistic regression模型建模之前執行相關性分析。如果不刪除相關變量，則很有可能誇大/錯誤地歸因變量對因變量的影響。邏輯回歸提供了一個可解釋的模型，並且它運行得更快，並且對於許多問題具有相似的準確性。 邏輯回歸的優點是模型簡單、易於訓練、推理速度快且可解釋，在關係簡單且線性的情況下，它是一個非常有吸引力的分類選擇。因此我們也加入了邏輯回歸分類器進實驗一起分析。最後得到下表結果：  

| Test Data Report   | Precision | Recall | F1-Score |
|  ----  | ----      | ----   |----       |
|  良性               | 0.96      | 0.99   |0.98     |
|  惡性               | 0.98     | 0.94   |0.96     |
|  accuracy          |           |        |0.97     |









